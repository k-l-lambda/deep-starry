{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dataset preparation\r\n",
    "from starry.topology.data import *\r\n",
    "\r\n",
    "\r\n",
    "file = open('./test/test.json', 'rb')\r\n",
    "data = loadConnectionSet(file)\r\n",
    "\r\n",
    "#seqs, matrixH, masks = exampleToTensors(data['connections'][0], 0x100, 0x200)\r\n",
    "#print(seqs, matrixH, masks)\r\n",
    "\r\n",
    "examples = list(map(lambda ex: exampleToTensors(ex, 0x100, 0x200), data['connections']))\r\n",
    "dataset = batchizeTensorExamples(examples, 4)\r\n",
    "\r\n",
    "print('dataset:', dataset[0]['seq_id'].shape, dataset[0]['seq_position'].shape, dataset[0]['mask'].shape, dataset[0]['matrixH'].shape)\r\n",
    "#print('seq_id.0', dataset[0]['seq_id'][0])\r\n",
    "#print('seq_position.0', dataset[0]['seq_position'][0][1])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sequence masking\r\n",
    "import torch\r\n",
    "from transformer.models import get_subsequent_mask, get_pad_mask\r\n",
    "\r\n",
    "\r\n",
    "seq = torch.tensor([[3,2,1], [4,5,6]])\r\n",
    "mask1 = get_pad_mask(seq, 1)\r\n",
    "mask2 = get_subsequent_mask(seq)\r\n",
    "print(mask1)\r\n",
    "print(mask2)\r\n",
    "print(mask1 & mask2)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model test\r\n",
    "from starry.topology.models import *\r\n",
    "\r\n",
    "\r\n",
    "model = TransformJointer()\r\n",
    "batch = dataset[0]\r\n",
    "pred = model(batch['seq_id'], batch['seq_position'], batch['mask'])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "\r\n",
    "\r\n",
    "t1 = torch.tensor([1,2,3])\r\n",
    "t2 = torch.tensor([4,5,6])\r\n",
    "\r\n",
    "t1 = t1.unsqueeze(0).repeat(2, 1, 1)\r\n",
    "t2 = t2.unsqueeze(1)\r\n",
    "print(t1, t2)\r\n",
    "\r\n",
    "p = t1.matmul(t2)\r\n",
    "print(p)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# predictor test\r\n",
    "from ipywidgets import interact_manual\r\n",
    "import json\r\n",
    "from starry.utils.config import Configuration\r\n",
    "from starry.topology.data import *\r\n",
    "from starry.topology.predictor import *\r\n",
    "\r\n",
    "\r\n",
    "def test (config, clusters):\r\n",
    "\tpredictor = Predictor(config)\r\n",
    "\tresults = predictor.predict(clusters)\r\n",
    "\ttext = json.dumps(results)\r\n",
    "\r\n",
    "\tprint('results:', text)\r\n",
    "\r\n",
    "\r\n",
    "def setConfig(config_dir, topology_file):\r\n",
    "\tconfig = Configuration(config_dir)\r\n",
    "\tprint('config loaded:', config.id)\r\n",
    "\r\n",
    "\tfile = open(topology_file, 'r')\r\n",
    "\tdata = loadClusterSet(file)\r\n",
    "\tclusters = data.get('clusters', data['connections'])[:4]\r\n",
    "\ttest(config, clusters)\r\n",
    "\r\n",
    "\r\n",
    "interact_manual(setConfig, config_dir='', topology_file='')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dump eval\r\n",
    "import json\r\n",
    "import dill as pickle\r\n",
    "from starry.utils.config import Configuration\r\n",
    "from starry.topology.data import Dataset\r\n",
    "from starry.topology.trainer import Trainer\r\n",
    "\r\n",
    "\r\n",
    "config = Configuration(r'.\\training\\20210809-chopin-20210807-l6+1')\r\n",
    "\r\n",
    "data_file = open(os.path.join(os.environ.get('DATA_DIR'), config['data.file_name']), 'rb')\r\n",
    "meta = pickle.load(data_file)\r\n",
    "print(meta['ids'][11])\r\n",
    "\r\n",
    "config['model.args.d_model'] = 0x200\r\n",
    "val, = Dataset.loadPackage(data_file, batch_size=1, splits='11/12', device='cpu')\r\n",
    "batch = next(iter(val))\r\n",
    "#print('batch:', batch['seq_id'])\r\n",
    "\r\n",
    "trainer = Trainer(config)\r\n",
    "deducer = trainer.model.deducer\r\n",
    "pred = deducer(batch['seq_id'], batch['seq_position'], batch['mask'])\r\n",
    "print('pred:', pred)\r\n",
    "\r\n",
    "output = {\r\n",
    "    'seq_id': batch['seq_id'].tolist(),\r\n",
    "    'seq_position': batch['seq_position'].tolist(),\r\n",
    "    'mask': batch['mask'].tolist(),\r\n",
    "    'pred': pred[0].tolist(),\r\n",
    "}\r\n",
    "json.dump(output, open('./test.json', 'w'))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit"
  },
  "interpreter": {
   "hash": "684b1123683431d89d3bfe9a89cc763215f4b8cd94b4aba1fb40ad45ff7c8b41"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}