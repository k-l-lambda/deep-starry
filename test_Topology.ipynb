{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dataset preparation\n",
    "from starry.topology.data import *\n",
    "\n",
    "\n",
    "file = open('./test/test.json', 'rb')\n",
    "data = loadConnectionSet(file)\n",
    "\n",
    "#seqs, matrixH, masks = exampleToTensors(data['connections'][0], 0x100, 0x200)\n",
    "#print(seqs, matrixH, masks)\n",
    "\n",
    "examples = list(map(lambda ex: exampleToTensors(ex, 0x100, 0x200), data['connections']))\n",
    "dataset = batchizeTensorExamples(examples, 4)\n",
    "\n",
    "print('dataset:', dataset[0]['seq_id'].shape, dataset[0]['seq_position'].shape, dataset[0]['mask'].shape, dataset[0]['matrixH'].shape)\n",
    "#print('seq_id.0', dataset[0]['seq_id'][0])\n",
    "#print('seq_position.0', dataset[0]['seq_position'][0][1])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sequence masking\n",
    "import torch\n",
    "from transformer.models import get_subsequent_mask, get_pad_mask\n",
    "\n",
    "\n",
    "seq = torch.tensor([[3,2,1], [4,5,6]])\n",
    "mask1 = get_pad_mask(seq, 1)\n",
    "mask2 = get_subsequent_mask(seq)\n",
    "print(mask1)\n",
    "print(mask2)\n",
    "print(mask1 & mask2)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model test\n",
    "from starry.topology.models import *\n",
    "\n",
    "\n",
    "model = TransformJointer()\n",
    "batch = dataset[0]\n",
    "pred = model(batch['seq_id'], batch['seq_position'], batch['mask'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "t1 = torch.tensor([1,2,3])\n",
    "t2 = torch.tensor([4,5,6])\n",
    "\n",
    "t1 = t1.unsqueeze(0).repeat(2, 1, 1)\n",
    "t2 = t2.unsqueeze(1)\n",
    "print(t1, t2)\n",
    "\n",
    "p = t1.matmul(t2)\n",
    "print(p)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# predictor test\n",
    "from ipywidgets import interact_manual\n",
    "import json\n",
    "from starry.utils.config import Configuration\n",
    "from starry.topology.data import *\n",
    "from starry.topology.predictor import *\n",
    "\n",
    "\n",
    "def test (config, clusters):\n",
    "\tpredictor = Predictor(config)\n",
    "\tresults = predictor.predict(clusters)\n",
    "\ttext = json.dumps(results)\n",
    "\n",
    "\tprint('results:', text)\n",
    "\n",
    "\n",
    "def setConfig(config_dir, topology_file):\n",
    "\tconfig = Configuration(config_dir)\n",
    "\tprint('config loaded:', config.id)\n",
    "\n",
    "\tfile = open(topology_file, 'r')\n",
    "\tdata = loadClusterSet(file)\n",
    "\tclusters = data.get('clusters', data['connections'])[:4]\n",
    "\ttest(config, clusters)\n",
    "\n",
    "\n",
    "interact_manual(setConfig, config_dir='', topology_file='')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dump eval\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import dill as pickle\n",
    "from starry.utils.config import Configuration\n",
    "from starry.topology.data import Dataset\n",
    "from starry.topology.trainer import Trainer\n",
    "\n",
    "\n",
    "config = Configuration(r'.\\training\\20210809-chopin-20210807-l6+1')\n",
    "\n",
    "data_file = open(os.path.join(os.environ.get('DATA_DIR'), config['data.file_name']), 'rb')\n",
    "meta = pickle.load(data_file)\n",
    "print(meta['ids'][11])\n",
    "\n",
    "config['model.args.d_model'] = 0x200\n",
    "val, = Dataset.loadPackage(data_file, batch_size=1, splits='11/12', device='cpu')\n",
    "batch = next(iter(val))\n",
    "#print('batch:', batch['seq_id'])\n",
    "\n",
    "trainer = Trainer(config)\n",
    "trainer.model.eval()\n",
    "deducer = trainer.model.deducer\n",
    "with torch.no_grad():\n",
    "    pred = deducer(batch['seq_id'], batch['seq_position'], batch['mask'])\n",
    "print('pred:', pred)\n",
    "\n",
    "output = {\n",
    "    'seq_id': batch['seq_id'].tolist(),\n",
    "    'seq_position': batch['seq_position'].tolist(),\n",
    "    'mask': batch['mask'].tolist(),\n",
    "    'pred': pred[0].tolist(),\n",
    "}\n",
    "json.dump(output, open('./test.json', 'w'))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test eval\n",
    "import os\n",
    "import torch\n",
    "import dill as pickle\n",
    "from starry.utils.config import Configuration\n",
    "from starry.topology.data import Dataset\n",
    "from starry.topology.trainer import Trainer\n",
    "\n",
    "\n",
    "config = Configuration(r'.\\training\\20210809-chopin-20210807-l6+1')\n",
    "\n",
    "data_file = open(os.path.join(os.environ.get('DATA_DIR'), config['data.file_name']), 'rb')\n",
    "meta = pickle.load(data_file)\n",
    "print(meta['ids'][11])\n",
    "\n",
    "config['model.args.d_model'] = 0x200\n",
    "val, = Dataset.loadPackage(data_file, batch_size=2, splits='11/12', device='cpu')\n",
    "\n",
    "\n",
    "trainer = Trainer(config)\n",
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val):\n",
    "        loss, acc = trainer.model(batch)\n",
    "        #print('loss:', loss)\n",
    "        print('acc:', acc)\n",
    "\n",
    "        if i > 10:\n",
    "            break\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove deducer in checkpoint state_dict\n",
    "from ipywidgets import interact_manual\n",
    "import torch\n",
    "from starry.utils.config import Configuration\n",
    "from starry.utils.model_factory import loadModel\n",
    "\n",
    "\n",
    "def run(config_dir):\n",
    "\tconfig = Configuration(config_dir)\n",
    "\tif config['best'] is None:\n",
    "\t\tprint('no best field found')\n",
    "\n",
    "\tconfig['model.type'] += 'Loss'\n",
    "\tmodel = loadModel(config['model'])\n",
    "\n",
    "\tcp_path = config.localPath(config['best'])\n",
    "\tcheckpoint = torch.load(cp_path, map_location='cpu')\n",
    "\tmodel.load_state_dict(checkpoint['model'])\n",
    "\n",
    "\tcheckpoint = {'epoch': checkpoint['epoch'], 'model': model.deducer.state_dict()}\n",
    "\ttorch.save(checkpoint, cp_path)\n",
    "\n",
    "\tprint('checkpoint saved:', cp_path)\n",
    "\n",
    "\n",
    "interact_manual(run, config_dir='')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}