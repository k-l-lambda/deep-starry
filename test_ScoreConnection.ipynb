{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dataset preparation\n",
    "from starry.score_connection.data import *\n",
    "\n",
    "\n",
    "file = open('./test/test.json', 'rb')\n",
    "data = loadConnectionSet(file)\n",
    "\n",
    "#seqs, matrixH, masks = exampleToTensors(data['connections'][0], 0x100, 0x200)\n",
    "#print(seqs, matrixH, masks)\n",
    "\n",
    "examples = list(map(lambda ex: exampleToTensors(ex, 0x100, 0x200), data['connections']))\n",
    "dataset = batchizeTensorExamples(examples, 4)\n",
    "\n",
    "print('dataset:', dataset[0]['seq_id'].shape, dataset[0]['seq_position'].shape, dataset[0]['mask'].shape, dataset[0]['matrixH'].shape)\n",
    "#print('seq_id.0', dataset[0]['seq_id'][0])\n",
    "#print('seq_position.0', dataset[0]['seq_position'][0][1])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sequence masking\n",
    "import torch\n",
    "from starry.transformer.models import get_subsequent_mask, get_pad_mask\n",
    "\n",
    "\n",
    "seq = torch.tensor([[3,2,1], [4,5,6]])\n",
    "mask1 = get_pad_mask(seq, 1)\n",
    "mask2 = get_subsequent_mask(seq)\n",
    "print(mask1)\n",
    "print(mask2)\n",
    "print(mask1 & mask2)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model test\n",
    "from os.path import expanduser\n",
    "import dill as pickle\n",
    "from starry.score_connection.models import *\n",
    "from starry.score_connection.data import Dataset\n",
    "\n",
    "\n",
    "data = pickle.load(open(expanduser('~/data/score/connections/packages/chopin10-3x100.pkl'), 'rb'))\n",
    "test, = Dataset.loadPackage(data, batch_size=1, splits='0/100')\n",
    "\n",
    "model = TransformJointerLoss()\n",
    "batch = next(iter(test))\n",
    "#pred = model(batch['seq_id'], batch['seq_position'], batch['mask'])\n",
    "#print(pred)\n",
    "\n",
    "loss, accuracy = model(batch)\n",
    "print('loss:', loss)\n",
    "print('accuracy:', accuracy)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# matmul test\n",
    "import torch\n",
    "\n",
    "\n",
    "t1 = torch.tensor([[1,2,3], [1,2,3]])\n",
    "t2 = torch.tensor([[4,5,6], [4,5,6]])\n",
    "\n",
    "t1 = t1.unsqueeze(-2)#.repeat(2, 2, 1, 1)\n",
    "t2 = t2.unsqueeze(-1)#.repeat(2, 2, 1, 1)\n",
    "print(t1, t2)\n",
    "\n",
    "p = t1.matmul(t2).flatten()\n",
    "print(p)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# repeat test\n",
    "import torch\n",
    "\n",
    "\n",
    "t = torch.tensor([[1,2,3], [1,2,3], [1,2,3]])\n",
    "r1 = t.repeat(2, 1, 1)\n",
    "r2 = t.unsqueeze(1).repeat(1, 2, 1)\n",
    "\n",
    "print(r1)\n",
    "print(r2)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# preprocessDataset test\n",
    "from starry.score_connection.data import *\n",
    "\n",
    "\n",
    "datasets = preprocessDataset('~/data/score/connections/chopin10-3x100', splits=('0/20',), batch_size=4)\n",
    "print('datasets:', len(datasets[0]), datasets[0][0])\n",
    "\n",
    "#import dill as pickle\n",
    "#pickle.dump(datasets[0], open('test.pkl', 'wb'))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dataset loading\n",
    "import dill as pickle\n",
    "from starry.score_connection.data import Dataset\n",
    "\n",
    "\n",
    "data = pickle.load(open('~/data/score/connections/packages/chopin10-3x100.pkl', 'rb'))\n",
    "training, validation = Dataset.loadPackage(data, batch_size=4, splits='*1,2/100;0/100')\n",
    "\n",
    "print('training len:', len(training))\n",
    "print('validation len:', len(validation))\n",
    "\n",
    "for batch in training:\n",
    "\tprint('batch:', batch)\n",
    "\tbreak\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# prediction\n",
    "import torch\n",
    "from os.path import expanduser\n",
    "import dill as pickle\n",
    "from starry.score_connection.models import *\n",
    "from starry.score_connection.data import Dataset\n",
    "\n",
    "\n",
    "checkpoint = torch.load('./output/model_01.chkpt', map_location='cpu')\n",
    "model = TransformJointer()\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "data = pickle.load(open(expanduser('~/data/score/connections/packages/chopin10-3x100.pkl'), 'rb'))\n",
    "test, = Dataset.loadPackage(data, batch_size=1, splits='0/100')\n",
    "\n",
    "batch = next(iter(test))\n",
    "#print('batch:', batch)\n",
    "\n",
    "pred = model(batch['seq_id'], batch['seq_position'], batch['mask'])\n",
    "matrixH = batch['matrixH']\n",
    "\n",
    "print('target:', matrixH)\n",
    "print('pred:', pred)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}